{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgregoire\\AppData\\Local\\Temp\\ipykernel_32336\\718647636.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  nn_model.load_state_dict(torch.load(model_filename))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ContextUnet:\n\tMissing key(s) in state_dict: \"init_conv.shortcut.weight\", \"init_conv.shortcut.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../weights/model_trained_500_LINEAR_100_32_context.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m nn_model \u001b[38;5;241m=\u001b[39m ContextUnet(in_channels, n_feat\u001b[38;5;241m=\u001b[39mn_feat, n_cfeat\u001b[38;5;241m=\u001b[39mn_cfeat, height\u001b[38;5;241m=\u001b[39mheight)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m nn_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#nn_model.train()\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#torch.save(nn_model, \"model.pth\")\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#m = torch.load(\"model.pth\")\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#m.eval()\u001b[39;00m\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ContextUnet:\n\tMissing key(s) in state_dict: \"init_conv.shortcut.weight\", \"init_conv.shortcut.bias\". "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src') # to be able to import functions from src\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import logging as log\n",
    "from unet import *\n",
    "from noise_scheduler import *\n",
    "from sampler import *\n",
    "from ddpm_sampler import *\n",
    "from ddim_sampler import *\n",
    "\n",
    "# network hyperparameters\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "in_channels=3 # rgb\n",
    "\n",
    "model_filename = f\"../weights/model_trained_500_LINEAR_100_32_context.pth\"\n",
    "nn_model = ContextUnet(in_channels, n_feat=n_feat, n_cfeat=n_cfeat, height=height)\n",
    "nn_model.load_state_dict(torch.load(model_filename))\n",
    "nn_model.eval()\n",
    "\n",
    "#nn_model.train()\n",
    "#torch.save(nn_model, \"model.pth\")\n",
    "#m = torch.load(\"model.pth\")\n",
    "#m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx.__version__='1.16.2', opset=21, IR_VERSION=10\n"
     ]
    }
   ],
   "source": [
    "from onnx import __version__, IR_VERSION\n",
    "from onnx.defs import onnx_opset_version\n",
    "print(f\"onnx.__version__={__version__!r}, opset={onnx_opset_version()}, IR_VERSION={IR_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OnnxExporterError",
     "evalue": "Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter.py:1509\u001b[0m, in \u001b[0;36mdynamo_export\u001b[1;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_export_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter.py:1236\u001b[0m, in \u001b[0;36mExporter.export\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdiagnostic_context, decomposition_skip\u001b[38;5;241m.\u001b[39menable_decomposition_skips(\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\n\u001b[0;32m   1233\u001b[0m ), torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpatch(\n\u001b[0;32m   1234\u001b[0m     dataclasses\u001b[38;5;241m.\u001b[39masdict(DEFAULT_EXPORT_DYNAMO_CONFIG)\n\u001b[0;32m   1235\u001b[0m ):\n\u001b[1;32m-> 1236\u001b[0m     graph_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_tracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;66;03m# TODO: Defer `import onnxscript` out of `import torch` path\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/103764\u001b[39;00m\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\_internal\\fx\\dynamo_graph_extractor.py:214\u001b[0m, in \u001b[0;36mDynamoExport.generate_fx\u001b[1;34m(self, options, model, model_args, model_kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fake_mode:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     graph_module, graph_guard \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracing_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfx_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m graph_guard  \u001b[38;5;66;03m# Unused\u001b[39;00m\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:1379\u001b[0m, in \u001b[0;36mexport.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1379\u001b[0m     result_traced \u001b[38;5;241m=\u001b[39m \u001b[43mopt_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConstraintViolationError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\_dynamo\\external_utils.py:38\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\_internal\\fx\\dynamo_graph_extractor.py:169\u001b[0m, in \u001b[0;36m_wrap_model_with_output_adapter.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(model_func)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_adapter\u001b[38;5;241m.\u001b[39mapply(\u001b[43mmodel_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, model\u001b[38;5;241m=\u001b[39mmodel)\n",
      "\u001b[1;31mTypeError\u001b[0m: ContextUnet.forward() missing 1 required positional argument: 't'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m500\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m      3\u001b[0m eps \u001b[38;5;241m=\u001b[39m nn_model(samples, t)\n\u001b[1;32m----> 4\u001b[0m onnx_program \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamo_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter.py:1520\u001b[0m, in \u001b[0;36mdynamo_export\u001b[1;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[0;32m   1512\u001b[0m resolved_export_options\u001b[38;5;241m.\u001b[39mdiagnostic_context\u001b[38;5;241m.\u001b[39mdump(sarif_report_path)\n\u001b[0;32m   1513\u001b[0m message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to export the model to ONNX. Generating SARIF report at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msarif_report_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1515\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSARIF is a standard format for the output of static analysis tools. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1518\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease report a bug on PyTorch Github: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_PYTORCH_GITHUB_ISSUES_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1519\u001b[0m )\n\u001b[1;32m-> 1520\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OnnxExporterError(\n\u001b[0;32m   1521\u001b[0m     ONNXProgram\u001b[38;5;241m.\u001b[39m_from_failure(e, resolved_export_options\u001b[38;5;241m.\u001b[39mdiagnostic_context),\n\u001b[0;32m   1522\u001b[0m     message,\n\u001b[0;32m   1523\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOnnxExporterError\u001b[0m: Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues"
     ]
    }
   ],
   "source": [
    "samples = torch.randn(9, 3, height, height) \n",
    "t = torch.tensor([1 / 500])[:, None, None, None]\n",
    "eps = nn_model(samples, t)\n",
    "onnx_program = torch.onnx.dynamo_export(nn_model, (samples, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator 'aten::uniform' to ONNX opset version 10 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Export the model   \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# model being run \u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m         \u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# model input (or a tuple for multiple inputs) \u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmymodel.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# where to save the model  \u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# store the trained parameter weights inside the model file \u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m         \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# the ONNX version to export the model to \u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# whether to execute constant folding for optimization \u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m         \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msamples\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# the model's input names \u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m         \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodelOutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# the model's output names \u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m \n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\utils.py:551\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExport destination must be specified for torchscript-onnx export.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    549\u001b[0m     )\n\u001b[1;32m--> 551\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\utils.py:1648\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m   1645\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1646\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m-> 1648\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1663\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[0;32m   1664\u001b[0m )\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\utils.py:1174\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[0;32m   1171\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1174\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1185\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch IR graph at exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph)\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\utils.py:714\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[1;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[0;32m    711\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[0;32m    712\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m--> 714\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    715\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m    716\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_lint(graph)\n",
      "File \u001b[1;32md:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\onnx\\utils.py:2007\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[1;34m(graph, block, node, inputs, env, values_in_env, new_nodes, operator_export_type)\u001b[0m\n\u001b[0;32m   2003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2004\u001b[0m         \u001b[38;5;66;03m# Clone node to trigger ONNX shape inference\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m graph_context\u001b[38;5;241m.\u001b[39mop(op_name, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattrs, outputs\u001b[38;5;241m=\u001b[39mnode\u001b[38;5;241m.\u001b[39moutputsSize())  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m-> 2007\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedOperatorError(\n\u001b[0;32m   2008\u001b[0m         symbolic_function_name,\n\u001b[0;32m   2009\u001b[0m         opset_version,\n\u001b[0;32m   2010\u001b[0m         symbolic_function_group\u001b[38;5;241m.\u001b[39mget_min_supported()\n\u001b[0;32m   2011\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m symbolic_function_group\n\u001b[0;32m   2012\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2013\u001b[0m     )\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m   2016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m operator_export_type \u001b[38;5;241m==\u001b[39m _C_onnx\u001b[38;5;241m.\u001b[39mOperatorExportTypes\u001b[38;5;241m.\u001b[39mONNX_FALLTHROUGH:\n",
      "\u001b[1;31mUnsupportedOperatorError\u001b[0m: Exporting the operator 'aten::uniform' to ONNX opset version 10 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues."
     ]
    }
   ],
   "source": [
    "# Export the model   \n",
    "torch.onnx.export(nn_model,         # model being run \n",
    "         (samples, t),       # model input (or a tuple for multiple inputs) \n",
    "         \"mymodel.onnx\",       # where to save the model  \n",
    "         export_params=True,  # store the trained parameter weights inside the model file \n",
    "         opset_version=10,    # the ONNX version to export the model to \n",
    "         do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "         input_names = ['samples', 'timestamp'],   # the model's input names \n",
    "         output_names = ['modelOutput'], # the model's output names \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to simple_cnn.onnx\n",
      "ONNX model is well formed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\bin\\miniforge3\\envs\\pytorch2\\Lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.onnx\n",
    "\n",
    "# Define a simple CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = nn.functional.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define a dummy input (adjust the size according to your model's input requirements)\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  dummy_input,         # model input (or a tuple for multiple inputs)\n",
    "                  \"simple_cnn.onnx\",   # where to save the model\n",
    "                  export_params=True,  # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,    # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'])\n",
    "\n",
    "print(\"Model exported to simple_cnn.onnx\")\n",
    "\n",
    "# Optionally, you can verify the model export:\n",
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"simple_cnn.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "print(\"ONNX model is well formed\")\n",
    "\n",
    "\n",
    "onnx_program = torch.onnx.dynamo_export(model, dummy_input)\n",
    "onnx_program.save(\"simple_cnn2.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ContextUnet(in_channels, n_feat=n_feat, n_cfeat=n_cfeat, height=height)\n",
    "samples = torch.randn(9, 3, height, height) \n",
    "t = torch.tensor([1 / 500])[:, None, None, None]\n",
    "torch.onnx.export(m,         # model being run \n",
    "         (samples, t),       # model input (or a tuple for multiple inputs) \n",
    "         \"mymodel.onnx\",       # where to save the model  \n",
    "         export_params=True,  # store the trained parameter weights inside the model file \n",
    "         opset_version=10,    # the ONNX version to export the model to \n",
    "         do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "         input_names = ['samples', 'timestamp'],   # the model's input names \n",
    "         output_names = ['modelOutput'], # the model's output names \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been converted to ONNX and saved at context_unet.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "from typing import Optional\n",
    "\n",
    "# First, let's copy the necessary class definitions from the provided code\n",
    "# (ResidualConvBlock, UnetUp, UnetDown, EmbedFC, ContextUnet)\n",
    "\n",
    "# ... [Copy all the class definitions here] ...\n",
    "\n",
    "# Now let's create a script to convert the model to ONNX\n",
    "\n",
    "def convert_to_onnx(model: ContextUnet, \n",
    "                    save_path: str, \n",
    "                    input_shape: tuple = (1, 3, 28, 28),\n",
    "                    time_shape: tuple = (1, 1),\n",
    "                    context_shape: Optional[tuple] = None):\n",
    "    \n",
    "    # Create dummy inputs\n",
    "    dummy_input = torch.randn(input_shape)\n",
    "    dummy_time = torch.randn(time_shape)\n",
    "    \n",
    "    if context_shape is not None:\n",
    "        dummy_context = torch.randn(context_shape)\n",
    "    else:\n",
    "        dummy_context = None\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Trace the model\n",
    "    traced_model = torch.jit.trace(model, (dummy_input, dummy_time, dummy_context))\n",
    "\n",
    "    # Export the model\n",
    "    torch.onnx.export(model,\n",
    "                      (dummy_input, dummy_time, dummy_context),\n",
    "                      save_path,\n",
    "                      export_params=True,\n",
    "                      opset_version=11,\n",
    "                      do_constant_folding=True,\n",
    "                      input_names=['input', 'time', 'context'],\n",
    "                      output_names=['output'],\n",
    "                      dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                    'time': {0: 'batch_size'},\n",
    "                                    'context': {0: 'batch_size'},\n",
    "                                    'output': {0: 'batch_size'}})\n",
    "\n",
    "    print(f\"Model has been converted to ONNX and saved at {save_path}\")\n",
    "    #onnx_program = torch.onnx.dynamo_export(model, (dummy_input, dummy_time, dummy_context))\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = ContextUnet(in_channels=3, n_feat=256, n_cfeat=10, height=28)\n",
    "    \n",
    "    # Convert to ONNX\n",
    "convert_to_onnx(model, \n",
    "                    save_path=\"context_unet.onnx\", \n",
    "                    input_shape=(1, 3, 28, 28),\n",
    "                    time_shape=(1, 1),\n",
    "                    context_shape=(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python output shape: torch.Size([1, 3, 28, 28])\n",
      "Scripted output shape: torch.Size([1, 3, 28, 28])\n",
      "Output difference: tensor(0.)\n",
      "Model has been converted to ONNX and saved at context_unet.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "from typing import Optional\n",
    "\n",
    "# [Include all the class definitions here as before]\n",
    "\n",
    "class ContextUnetWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, t, c):\n",
    "        return self.model(x, t, c)\n",
    "\n",
    "def convert_to_onnx(model: ContextUnet, \n",
    "                    save_path: str, \n",
    "                    input_shape: tuple = (1, 3, 28, 28),\n",
    "                    time_shape: tuple = (1, 1),\n",
    "                    context_shape: Optional[tuple] = None):\n",
    "    \n",
    "    # Create dummy inputs\n",
    "    dummy_input = torch.randn(input_shape)\n",
    "    dummy_time = torch.randn(time_shape)\n",
    "    \n",
    "    if context_shape is not None:\n",
    "        dummy_context = torch.randn(context_shape)\n",
    "    else:\n",
    "        dummy_context = None\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Wrap the model\n",
    "    wrapped_model = ContextUnetWrapper(model)\n",
    "\n",
    "    # Script the model\n",
    "    scripted_model = wrapped_model\n",
    "\n",
    "    # Debug: Compare outputs\n",
    "    with torch.no_grad():\n",
    "        python_output = model(dummy_input, dummy_time, dummy_context)\n",
    "        script_output = scripted_model(dummy_input, dummy_time, dummy_context)\n",
    "    \n",
    "    print(\"Python output shape:\", python_output.shape)\n",
    "    print(\"Scripted output shape:\", script_output.shape)\n",
    "    print(\"Output difference:\", torch.max(torch.abs(python_output - script_output)))\n",
    "\n",
    "    # Export the scripted model\n",
    "    torch.onnx.export(scripted_model,\n",
    "                      (dummy_input, dummy_time, dummy_context),\n",
    "                      save_path,\n",
    "                      export_params=True,\n",
    "                      opset_version=12,\n",
    "                      do_constant_folding=True,\n",
    "                      input_names=['input', 'time', 'context'],\n",
    "                      output_names=['output'],\n",
    "                      dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                    'time': {0: 'batch_size'},\n",
    "                                    'context': {0: 'batch_size'},\n",
    "                                    'output': {0: 'batch_size'}})\n",
    "                                    \n",
    "\n",
    "    print(f\"Model has been converted to ONNX and saved at {save_path}\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set a fixed seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = ContextUnet(in_channels=3, n_feat=256, n_cfeat=10, height=28)\n",
    "    \n",
    "    # Convert to ONNX\n",
    "    convert_to_onnx(model, \n",
    "                    save_path=\"context_unet.onnx\", \n",
    "                    input_shape=(1, 3, 28, 28),\n",
    "                    time_shape=(1, 1),\n",
    "                    context_shape=(1, 10))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape with residual connection: torch.Size([2, 3, 64, 64])\n",
      "Output shape without residual connection: torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Define input tensor with shape (batch_size, in_channels, height, width)\n",
    "batch_size = 2\n",
    "in_channels = 3\n",
    "out_channels = 3\n",
    "height = 64\n",
    "width = 64\n",
    "\n",
    "dummy_input = torch.randn(batch_size, in_channels, height, width)\n",
    "\n",
    "# Instantiate the block with and without residual connections\n",
    "block_res = ResidualConvBlock(in_channels, out_channels, is_res=True)\n",
    "block_no_res = ResidualConvBlock(in_channels, out_channels, is_res=False)\n",
    "\n",
    "# Pass the input tensor through the blocks\n",
    "output_res = block_res(dummy_input)\n",
    "output_no_res = block_no_res(dummy_input)\n",
    "\n",
    "# Print the output shapes\n",
    "print(f\"Output shape with residual connection: {output_res.shape}\")\n",
    "print(f\"Output shape without residual connection: {output_no_res.shape}\")\n",
    "\n",
    "torch.onnx.export(block_res,\n",
    "                    dummy_input,\n",
    "                    \"context_unet.onnx\",\n",
    "                    export_params=True,\n",
    "                    opset_version=12,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names=['input', 'time', 'context'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                'time': {0: 'batch_size'},\n",
    "                                'context': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})\n",
    "\n",
    "import onnx\n",
    "onnx_model = onnx.load(\"context_unet.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Define input tensors with appropriate shapes\n",
    "batch_size = 2\n",
    "in_channels = 128\n",
    "out_channels = 64\n",
    "h, w = 32, 32  # spatial dimensions of the input\n",
    "h_skip, w_skip = 32, 32  # spatial dimensions of the skip connection\n",
    "\n",
    "# Create dummy input tensors\n",
    "x = torch.randn(batch_size, in_channels, h, w)  # input tensor\n",
    "skip = torch.randn(batch_size, out_channels, h_skip, w_skip)  # skip connection tensor\n",
    "\n",
    "# Instantiate the UnetUp block\n",
    "up_block = UnetUp(in_channels + out_channels, out_channels)\n",
    "\n",
    "# Pass the tensors through the block\n",
    "output = up_block(x, skip)\n",
    "\n",
    "# Print the output shape\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "torch.onnx.export(up_block,\n",
    "                    (x, skip),\n",
    "                    \"context_unet.onnx\",\n",
    "                    export_params=True,\n",
    "                    opset_version=12,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names=['input', 'time', 'context'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                'time': {0: 'batch_size'},\n",
    "                                'context': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})\n",
    "\n",
    "import onnx\n",
    "onnx_model = onnx.load(\"context_unet.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Define input tensors with appropriate shapes\n",
    "batch_size = 2\n",
    "in_channels = 128\n",
    "out_channels = 64\n",
    "h, w = 32, 32  # spatial dimensions of the input\n",
    "\n",
    "# Create a dummy input tensor\n",
    "x = torch.randn(batch_size, in_channels, h, w)\n",
    "\n",
    "# Instantiate the UnetDown block\n",
    "down_block = UnetDown(in_channels, out_channels)\n",
    "\n",
    "# Pass the tensor through the block\n",
    "output = down_block(x)\n",
    "\n",
    "# Print the output shape\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "torch.onnx.export(down_block,\n",
    "                    x,\n",
    "                    \"context_unet.onnx\",\n",
    "                    export_params=True,\n",
    "                    opset_version=12,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names=['input', 'time', 'context'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                'time': {0: 'batch_size'},\n",
    "                                'context': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})\n",
    "\n",
    "import onnx\n",
    "onnx_model = onnx.load(\"context_unet.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 20])\n"
     ]
    }
   ],
   "source": [
    "# Define input tensor with appropriate shape\n",
    "batch_size = 4\n",
    "input_dim = 10  # dimensionality of the input\n",
    "emb_dim = 20    # dimensionality of the embedding space\n",
    "\n",
    "# Create a dummy input tensor\n",
    "x = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# Instantiate the EmbedFC block\n",
    "embed_fc = EmbedFC(input_dim, emb_dim)\n",
    "\n",
    "# Pass the tensor through the block\n",
    "output = embed_fc(x)\n",
    "\n",
    "# Print the output shape\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\n",
    "torch.onnx.export(embed_fc,\n",
    "                    x,\n",
    "                    \"context_unet.onnx\",\n",
    "                    export_params=True,\n",
    "                    opset_version=12,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names=['input', 'time', 'context'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                'time': {0: 'batch_size'},\n",
    "                                'context': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})\n",
    "\n",
    "import onnx\n",
    "onnx_model = onnx.load(\"context_unet.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class tmpContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(tmpContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample  \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                       \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        log.debug(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define input parameters\n",
    "batch_size = 4\n",
    "in_channels = 3       # Number of input channels (e.g., RGB image)\n",
    "height = 28           # Height and width of the input image (assumed square)\n",
    "n_feat = 256          # Number of intermediate feature maps\n",
    "n_cfeat = 10          # Number of context features\n",
    "\n",
    "# Create dummy input tensors\n",
    "x = torch.randn(batch_size, in_channels, height, height)  # Input image tensor\n",
    "t = torch.randn(batch_size, 1)                            # Time step tensor\n",
    "c = torch.randn(batch_size, n_cfeat)                      # Context tensor\n",
    "\n",
    "# Instantiate the ContextUnet model\n",
    "context_unet = ContextUnet(in_channels, n_feat=n_feat, n_cfeat=n_cfeat, height=height)\n",
    "\n",
    "# Pass the tensors through the model\n",
    "output = context_unet(x,t,c)\n",
    "\n",
    "# Print the output shape\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "torch.onnx.export(context_unet,\n",
    "                    (x,t,c),\n",
    "                    \"context_unet.onnx\",\n",
    "                    export_params=True,\n",
    "                    opset_version=12,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names=['input', 'time', 'context'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                'time': {0: 'batch_size'},\n",
    "                                'context': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})\n",
    "\n",
    "import onnx\n",
    "onnx_model = onnx.load(\"context_unet.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
